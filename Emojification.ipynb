{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating  Emojis for a Given Sentence Using Deep Learning:\n",
    "This jupyter notebook contains code to generate appropriate emojis using Deep Learning techniques. The notebook has 2 parts:<br>\n",
    "1) Generating Emojis using Simple one-layered Neural Network.<br>\n",
    "2) Generating Emojis using Recurrent Neural Network with Long Short Term Memory (LSTM).<br>\n",
    "Glove Vectors: This Project Uses Glove Vectors from https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing important packages.\n",
    "import numpy as np\n",
    "import emoji\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Training And Test Data files:\n",
    "We will read our training and test set from a csv file which has sentences in first column and corresponding labels in the second column.<br> Labels are integers from 0-4.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the training set csv file using pandas.\n",
    "Data = pd.read_csv(\"Training_set.csv\")\n",
    "# To drop, if any, the columns with Null values.\n",
    "Data = Data.dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting Pandas Dataframe into numpy array. \n",
    "Data1 = Data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The first column of the file has Sentences. Saving them as strings in a numpy array.\n",
    "X_train = Data1[:,0]\n",
    "# The second column has labels which are codes for the emoji to be used. Label 0 means heart, label 1 means ball etc.\n",
    "Y_train = Data1[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the test set csv file using pandas.\n",
    "Data2 = pd.read_csv(\"Test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting Pandas Dataframe into numpy array.\n",
    "Data3 = Data2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The first column of the file has Sentences. Saving them as strings in a numpy array.\n",
    "X_test = Data3[:,0]\n",
    "# The second column has labels which are codes for the emoji to be used. Label 0 means heart, label 1 means ball etc.\n",
    "Y_test = Data3[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Finding the length of the Longest Sentence in the Training Set.\n",
    "maxLen = len(max(X_train, key=len).split(' '))\n",
    "print(maxLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Labels to Emoji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a dictionary having label : emoji code as key:value pairs.\n",
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    \n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}\n",
    "# Converting label to emoji using emoji package.\n",
    "def label_to_emoji(label):\n",
    "    \"\"\"Function Paramters: label: A label between 0 and 4.\n",
    "       Return: Function returns an emoji emoticon corresponding to a label.\"\"\"\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Labels to One-Hot Vectors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting labels to one-hot vectors as the softmax cross-entropy loss function uses this format of labels.\n",
    "def convert_to_one_hot(Y):\n",
    "    \"\"\"Function Paramters: Y : A numpy array/vector of labels.\n",
    "       Return: Function Returns a numpy array of shape (m , maxlabel+1) where m is length of Y.\n",
    "       maxlabel + 1 means if we have max label as 4, we need 4 zeros and 1 one to represent it as one-hot.\"\"\"\n",
    "    \n",
    "    # Initializing one-hot numpy array\n",
    "    a = np.zeros((Y.shape[0],np.max(Y)+1))\n",
    "    # Storing 1 at index == label for an example.\n",
    "    a[np.arange(Y.shape[0]), Y] = 1\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting the Y_train labels to one-hot vectors.\n",
    "# Before using convert_to_one_hot(Y) function, we need to convert Y_train datatype to integers.\n",
    "Y_train = Y_train.astype(int)\n",
    "# calling the above function.\n",
    "Y_one_hot_train = convert_to_one_hot(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is converted to one-hot vector [ 1.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Let us check.\n",
    "print(str(Y_train[5]) + \" is converted to one-hot vector \" + str(Y_one_hot_train[5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting the Y_test labels to one-hot vectors.\n",
    "# Before using convert_to_one_hot(Y) function, we need to convert Y_test datatype to integers.\n",
    "Y_test = Y_test.astype(int)\n",
    "# Calling the above function.\n",
    "Y_one_hot_test = convert_to_one_hot(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 is converted to one-hot vector [ 0.  0.  0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Let us check.\n",
    "print(str(Y_test[11]) + \" is converted to one-hot vector \" + str(Y_one_hot_test[11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reading Word Embeddings from Glove Vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that reads glove vector file and returns words_to_index, index_to_words, word_to_vec_map.\n",
    "def read_glove_vectors(glove_file):\n",
    "    \"\"\"Function Paramters: Path to Glove Vector File.\n",
    "       Return: Function Returns:\n",
    "       1) words_to_index: A dictionary containing mapping from words to index.\n",
    "       2) index_to_words: A dictionary containing mapping from index to words.\n",
    "       3) word_to_vec_map: A dictionary conataining mapping from words to vectors. \"\"\"\n",
    "    # Open the text file and create an object f of the file.\n",
    "    with open(glove_file, 'r') as f:\n",
    "        # Creating an empty set that will contain words.\n",
    "        words = set()\n",
    "        # Creating an empty dictionary that will contain word:vector pairs.\n",
    "        word_to_vec_map = {}\n",
    "        # Looping through the file line by line. Every line has a word followed by it's vector.\n",
    "        for line in f:\n",
    "            # Splitting the long string of word followed by vector values. around blank spaces i.e. taking out words from a line.\n",
    "            line = line.strip().split()\n",
    "            # Taking out the word i.e. first element of the line.\n",
    "            curr_word = line[0]\n",
    "            # Adding current word to the set.\n",
    "            words.add(curr_word)\n",
    "            # Adding word:vector pair to the dictionary.\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        # Creating index to word and word to index dictionaries.  \n",
    "        # Starting indexes of words from 1.\n",
    "        i = 1\n",
    "        # Creating empty dictionaries.\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        # Looping through all the words created in the set above.\n",
    "        for w in sorted(words):\n",
    "            # Saving word:index pair in the dictionary.\n",
    "            words_to_index[w] = i\n",
    "            # Saving index:word pair in the dictionary.\n",
    "            index_to_words[i] = w\n",
    "            # Increamenting index by 1 for the next word.\n",
    "            i = i + 1\n",
    "        \n",
    "        return words_to_index, index_to_words, word_to_vec_map    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vectors('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of chilly in the vocabulary is 99074\n",
      "the 289846th word in the vocabulary is potatos\n"
     ]
    }
   ],
   "source": [
    "word = \"chilly\"\n",
    "index = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Simple Model to Predict Label From Sentence:\n",
    "This Model uses all the words in the Sentence and takes average of their Word Vectors to predict label for the sentence using a 1 Layered Neural Network with Softmax Activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes a sentence as an input and returns the average of word vectors of the words in that sentence.\n",
    "def average(sentence , word_to_vec_map):\n",
    "    \"\"\"Function Parameters: sentence: A sentence in the form of a string.\n",
    "       word_to_vec_map: A dictionary containing words to vector mapping.\n",
    "       Return: Function returns a vector with average values of the embedding vectors.\"\"\"\n",
    "    # Splitting Sentence into lowercase words.\n",
    "    words = [i.lower() for i in sentence.split()]\n",
    "    \n",
    "    total = 0\n",
    "    for i in words:\n",
    "        total = total + word_to_vec_map[i]\n",
    "    \n",
    "    \n",
    "    average = total / len(words)\n",
    "    \n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.40837067,  0.74165   , -0.78191333, -0.15946333,  0.76338333,\n",
       "        0.45639133, -0.4019587 , -0.08766333, -0.04882333,  0.20158667,\n",
       "        0.00542   ,  0.14020673,  0.07584033, -0.11334333,  0.51032333,\n",
       "        0.136538  ,  0.11707   ,  0.53019667, -0.071368  , -0.18856333,\n",
       "       -0.25945333,  0.82907667,  0.11644533,  0.30177   ,  0.85965333,\n",
       "       -1.70628   , -0.84639667,  0.70187   ,  0.25922133, -0.57696467,\n",
       "        3.07303333, -0.30657667,  0.00867667, -0.33548533,  0.07780833,\n",
       "        0.0226    ,  0.07458067,  0.54815   , -0.08488   , -0.67778333,\n",
       "       -0.079656  ,  0.12986   ,  0.0259089 , -0.295027  , -0.09114533,\n",
       "       -0.10266633, -0.05054967, -1.05132333,  0.198019  ,  0.29884033])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us check\n",
    "average(\"She is beautiful\", word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"Function Paramter: z: a vector or a python variable\n",
    "       Return: Function returns softmax of the input vector.\"\"\"\n",
    "    # We subtract so that value doesn't becomes infinity.\n",
    "    e_z = np.exp(z - np.max(z))\n",
    "    \n",
    "    return e_z / e_z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training a Simple Neural Network Model. \n",
    "def model(X ,Y , word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "    \"\"\"Function Paramters: X: A numpy array of shape(m,1) having sentences\n",
    "       Y: Numpy vector of labels \n",
    "       word_to_vec_map: A dictionary containing word to vector mapping\n",
    "       learning_rate: for gradient descent\n",
    "       num_iteration: Number of iterations for Gradient Descent.\n",
    "       Return: Function returns the updated paramters W and b\"\"\"\n",
    "\n",
    "    # Number of training examples.\n",
    "    m = Y.shape[0]\n",
    "    \n",
    "    # Number of output nodes.\n",
    "    n_y = 5\n",
    "    \n",
    "    # Number of input nodes i.e. length of Glove Vector.\n",
    "    n_h = 50\n",
    "    \n",
    "    # Parameter Initialization using Xavier Technique:\n",
    "    \n",
    "    W = np.random.randn(n_y , n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y , 1))\n",
    "    \n",
    "    # Converting Labels to one-hot Vectors.\n",
    "    Y = Y.astype(int)\n",
    "    Y_one_hot = convert_to_one_hot(Y)\n",
    "    \n",
    "    # Stochastic Gradient Descent:\n",
    "    for t in range(num_iterations):\n",
    "        # Looping over the examples one-by-one.\n",
    "        for i in range(m):\n",
    "            avg = average(X[i] , word_to_vec_map)\n",
    "            avg = avg.reshape(50,1)\n",
    "            # Forward Propagation.\n",
    "            Z = np.dot(W, avg) + b\n",
    "            \n",
    "            A = softmax(Z)\n",
    "            \n",
    "            # Cost For Softmax Function;\n",
    "            \n",
    "            cost = -(np.sum(np.multiply(Y_one_hot[i].reshape(5,1), np.log(A))))\n",
    "            \n",
    "            # Computing Gradients:\n",
    "            \n",
    "            # Derivative of cost w.r.t Z\n",
    "            dz = A - Y_one_hot[i].reshape(5,1)\n",
    "            # Derivative of cost w.r.t W . We do outer dot product between dz and avg.\n",
    "            dw = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            # Derivative of cost w.r.t b\n",
    "            db = dz\n",
    "            \n",
    "            # Stochastic Gradient Descent Update:\n",
    "            \n",
    "            W = W - learning_rate * dw\n",
    "            \n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "            \n",
    "            \"\"\"if t % 100 == 0:\n",
    "                print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "                pred = predict(X, Y, W, b, word_to_vec_map)\"\"\"\n",
    "        if(t%100 == 0 ):   \n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "    \n",
    "    return  W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.95204988128\n",
      "Epoch: 100 --- cost = 0.0797181872601\n",
      "Epoch: 200 --- cost = 0.0445636924368\n",
      "Epoch: 300 --- cost = 0.0343226737879\n"
     ]
    }
   ],
   "source": [
    "# Calling the function to train on Training Set.\n",
    "W, b = model(X_train, Y_train, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function predicts labels for Sentences using only Forward Propagation:\n",
    "def predict(X , Y , W, b, word_to_vec_map):\n",
    "    \"\"\"Function Parameters: X: A numpy array of shape(m,1) having sentences\n",
    "       Y: Numpy vector of labels \n",
    "       W,b: Trained Parameters\n",
    "       word_to_vec_map: A dictionary containing word to vector mapping\n",
    "       Return: Function returns Label predictions for all examples in X.\"\"\"\n",
    "    # Number of examples to be predicted\n",
    "    m = X.shape[0]\n",
    "    # Array of Zeros to store prediction labels\n",
    "    pred = np.zeros((m,1))\n",
    "    \n",
    "    # Looping over the training examples\n",
    "    for i in range(m):\n",
    "        avg = average(X[i] , word_to_vec_map)\n",
    "        # Forward Propagation:\n",
    "        \n",
    "        Z = np.dot(W, avg.reshape(50,1)) + b\n",
    "        A = softmax(Z)\n",
    "        \n",
    "        # Saving the prediction label in pred vector.\n",
    "        pred[i] = np.argmax(A)\n",
    "    \n",
    "    print(\"Accuracy = \"  + str(np.mean((pred == Y.reshape(Y.shape[0],1)))))\n",
    "        \n",
    "    return pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.977272727273\n"
     ]
    }
   ],
   "source": [
    "# Calling the function to make prediction on training set.\n",
    "pred = predict(X_train,Y_train, W , b , word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.833333333333\n"
     ]
    }
   ],
   "source": [
    "# Making prediction on unseen data.\n",
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generating Emojis using Recurrent Neural Network with Long Short Term Memory (LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing keras package and functionalities.\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Converting Sentences to Indices of Words:\n",
    "To Use Sequence Model like LSTM, we need to convert sentences into indices first and then eventually use word-embeddings for training.<br> \n",
    "<b> Note:</b> We will do padding with 0 vectors and make all sentences of same length. This is a requirement for training using a Sequence Model using a Batch Gradient Descent Approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices(X , word_to_index, maxLen):\n",
    "    \"\"\"Function Parameters: X: Numpy Array having sentences.\n",
    "       word_to_index: A dictionary mapping words to index.\n",
    "       maxLen: Length of Longest Sentence\n",
    "       Return: X_indices : A numpy array of shape (m,10) having indices for all 10 words in a sentence.\n",
    "       Index 0 means padded word.\"\"\"\n",
    "    \n",
    "    # Number of training examples.\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Initialize a X_indices matrix of shape (m , maxLen).We use maxLen for all sentences in training example to make \n",
    "    # them of equal length.\n",
    "    X_indices = np.zeros((m , maxLen))\n",
    "    \n",
    "    # Looping over all the training examples.\n",
    "    for i in range(m):\n",
    "        # Splitting a sentence into words.\n",
    "        # Create a list of words having all words in a sentence in lower case.\n",
    "        words = [j.lower() for j in X[i].split()]\n",
    "        \n",
    "        # Initialize word counter to set index as 0: \n",
    "        k = 0 \n",
    "        # Looping over words in a sentence: \n",
    "        for w in words:\n",
    "            # Setting i,jth index in X_indexes:\n",
    "            X_indices[i][k] = word_to_index[w]\n",
    "            # To set index of next word, increase k\n",
    "            k = k + 1 \n",
    "            \n",
    "     \n",
    "    return X_indices            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Keras Embedding Layer:\n",
    "Before building the LSTM Network, we need to create an Embedding layer in Keras which can convert word index into Word Vectors.<br>\n",
    "It is just like creating mapping from indices to vectors. Embedding Layer saves us from using expensive operations for fetching word vectors from indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map , word_to_index):\n",
    "    \"\"\"Function Parameters: word_to_vec_map : A dictionary mapping word to vectors.\n",
    "       word_to_index: A dictionary mapping word to index.\n",
    "       Return: Function returns a Keras embedding layer.\"\"\"\n",
    "    \n",
    "    # Length of vocabulary. + 1 is required by Keras. \n",
    "    # Because index for words start from 1 and not 0.\n",
    "    vocab_len = len(word_to_index) + 1 \n",
    "    # Dimension of embedding vector. We use 50 because our pretrained Glove vec has length 50\n",
    "    emb_dim = 50\n",
    "    \n",
    "    # Initializing an emb_matrix with Zeros. Every row will correspond to vector for that word.\n",
    "    emb_matrix = np.zeros((vocab_len , emb_dim))\n",
    "    \n",
    "    # Looping over each element of word_to_index and saving vectors row-wise in emb_matrix.\n",
    "    \n",
    "    for word,index in word_to_index.items():\n",
    "        \n",
    "        emb_matrix[index , :] = word_to_vec_map[word]\n",
    "        \n",
    "    \n",
    "    # Defining Keras Embedding Layer.This should have parmaters as Non-Trainable because we don't want to alter the embedding we are using.\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_len , emb_dim, trainable = False)\n",
    "    \n",
    "    # Before giving weights to embedding layer, we need to build the layer. \n",
    "    \n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Setting the weights for embedding layer.\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = [-1.05879998  0.26952001  0.94632     0.056907    0.24439999  0.37810001\n",
      "  1.32579994 -0.88515002 -0.31154999  0.57618999 -0.056118   -0.62589002\n",
      " -0.41668999 -0.58279002  0.66974998  0.11759     0.68662     0.62711\n",
      " -0.65701997 -0.078008   -0.52221     0.018973    0.97861999  0.78516001\n",
      "  0.69097     0.47174999 -1.1171      0.25342     0.34635001 -1.18659997\n",
      "  0.69871998  0.66864002 -1.27649999  0.92610002 -0.017565   -0.25185001\n",
      "  1.44840002 -0.75392997 -0.07427    -0.18682     0.69292998 -0.56638002\n",
      " -0.39572001 -0.30950999 -0.94393998  0.27484     1.06850004  0.31138\n",
      "  0.79843003  0.20392001]\n"
     ]
    }
   ],
   "source": [
    "# Embedding_layer can be indexed using 3 index viz 0, index, vector_index.\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][2][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Emoji_Model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emoji_lstm_model(input_shape , word_to_vec_map , word_to_index):\n",
    "    \"\"\"Function Paramters : input_shape: Shape of the input layer for Keras Model.\n",
    "       word_to_vec_map : Mapping from words to vectors.\n",
    "       word_to_index : Mapping from words to index.\n",
    "       Return : Function returns Keras model. \"\"\"\n",
    "    \n",
    "    # We will input the indices rather than words. Creating the input layer for the Network.\n",
    "    \n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    \n",
    "    # Creating the pre-trained embedding layer by calling the above function.\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map , word_to_index)\n",
    "    \n",
    "    # Propagating Input through Embedding layer to get word embedding vectors.\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # Creating LSTM layer with 128 dimensional hidden state:\n",
    "    # Also, returned sequence should be batch of sequences.\n",
    "    \n",
    "    X = LSTM(128 , return_sequences = True)(embeddings)\n",
    "    \n",
    "    # Adding Dropout Layer With Probability 0.5:\n",
    "    \n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Creating Second LSTM Layer with 128 Dimensional Hidden State. \n",
    "    # But now, return_sequences = False because we want output at only the last time step and not all time steps.\n",
    "    \n",
    "    X = LSTM(128 , return_sequences = False)(X)\n",
    "    \n",
    "    # Adding Dropout Layer With Probability 0.5:\n",
    "    \n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Creating Dense Layer at last time step returning a vector of size 5 with softmax activation.\n",
    "    \n",
    "    X = Dense(5)(X)\n",
    "    \n",
    "    X = Activation(\"softmax\")(X)\n",
    "    \n",
    "    # Creating Model Instance:\n",
    "    \n",
    "    model = Model(inputs = sentence_indices , outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calling the function to create model.\n",
    "model = emoji_lstm_model((10,) , word_to_vec_map , word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Printing summary of the model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling the model:\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 10)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "# We have already created one-hot from original labels of Y_train.\n",
    "Y_one_hot_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Dimensions:\n",
    "We pass X and Y of shape (m,10) and (m,5) respectively i.e. each row represents a training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 1.6138 - acc: 0.1591\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 957us/step - loss: 1.5321 - acc: 0.3333\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 959us/step - loss: 1.4712 - acc: 0.4015\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 985us/step - loss: 1.3875 - acc: 0.4924\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 994us/step - loss: 1.3028 - acc: 0.4924\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.1877 - acc: 0.5682\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.1180 - acc: 0.6212\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9839 - acc: 0.6515\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 990us/step - loss: 0.9572 - acc: 0.6591\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9274 - acc: 0.6591\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.7616 - acc: 0.7727\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 989us/step - loss: 0.7445 - acc: 0.7424\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.6283 - acc: 0.7955\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.6222 - acc: 0.7955\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.5570 - acc: 0.7955\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.5512 - acc: 0.8182\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.4880 - acc: 0.8485\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.4024 - acc: 0.8409\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3452 - acc: 0.8788\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2927 - acc: 0.9015\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3175 - acc: 0.9015\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3073 - acc: 0.8939\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3350 - acc: 0.8712\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2960 - acc: 0.9167\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1484 - acc: 0.9545\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2202 - acc: 0.9318\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1740 - acc: 0.9318\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1559 - acc: 0.9621\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0956 - acc: 0.9773\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0781 - acc: 0.9848\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1644 - acc: 0.9545\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1802 - acc: 0.9394\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1839 - acc: 0.9394\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 954us/step - loss: 0.1492 - acc: 0.9545\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3399 - acc: 0.8939\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2702 - acc: 0.9091\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2061 - acc: 0.9470\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1285 - acc: 0.9697\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1162 - acc: 0.9773\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1229 - acc: 0.9697\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0812 - acc: 0.9848\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0706 - acc: 0.9697\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 974us/step - loss: 0.2231 - acc: 0.9394\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 993us/step - loss: 0.2045 - acc: 0.9242\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1938 - acc: 0.9167\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 955us/step - loss: 0.2676 - acc: 0.8864\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 972us/step - loss: 0.1014 - acc: 0.9545\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 966us/step - loss: 0.1294 - acc: 0.9394\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0725 - acc: 0.9924\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 951us/step - loss: 0.0821 - acc: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x115a48e48>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model on the training set.\n",
    "model.fit(X_train_indices, Y_one_hot_train, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Accuracy on the Test Set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 268us/step\n",
      "Test accuracy =  0.857142857143\n"
     ]
    }
   ],
   "source": [
    "# Converting Sentences to Indices.\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "# Evaluating model on the Test set\n",
    "loss, acc = model.evaluate(X_test_indices, Y_one_hot_test)\n",
    "\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Mis-labelled Examples in the Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:üòÑ prediction: he got a very nice raise‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: she got me a nice present‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: This girl is messing with me‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: you brighten my day‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: she is a bully‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: My life is so boring‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: will you be my valentine‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: What you did was awesomeüòû\n"
     ]
    }
   ],
   "source": [
    "# Finding predictions on the Test Set.\n",
    "# Predictions are the Output of the Network i.e. Probability Vector.\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    # Finding the label from probability vector.\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Emoji for a New Sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi üòÑ\n"
     ]
    }
   ],
   "source": [
    "# Converting user input sentence to numpy array. \n",
    "x_test = np.array([text])\n",
    "# Converting words to indices. \n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
